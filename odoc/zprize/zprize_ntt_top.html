<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>zprize_ntt_top (zprize.zprize_ntt_top)</title><link rel="stylesheet" href="../odoc.css"/><meta charset="utf-8"/><meta name="generator" content="odoc 2.1.1"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><script src="../highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script></head><body class="odoc"><nav class="odoc-nav"><a href="index.html">Up</a> â€“ <a href="index.html">zprize</a> &#x00BB; zprize_ntt_top</nav><header class="odoc-preamble"><h2 id="zprize_ntt"><a href="#zprize_ntt" class="anchor"></a>Zprize_ntt</h2><p><a href="Zprize_ntt/index.html">This library</a> provides a design which performs a single transform size configured at build time. For the ZPrize competition we target a transform of size 2<sup>24</sup>.</p></header><nav class="odoc-toc"><ul><li><a href="#algorithm">Algorithm</a></li><li><a href="#our-implementation">Our Implementation</a><ul><li><a href="#2-phases-to-compute-the-ntt">2 Phases to Compute the NTT</a></li></ul></li><li><a href="#memory-bandwidth-and-streaming">Memory Bandwidth and Streaming</a></li><li><a href="#experiments">Experiments</a><ul><li><a href="#normal-layout-builds">Normal-layout Builds</a></li><li><a href="#optimized-layout-builds">Optimized-layout Builds</a></li></ul></li><li><a href="#results-(for-competition-criteria)">Results (For Competition Criteria)</a><ul><li><a href="#latency,-power-and-resource-utilization">Latency, Power and Resource Utilization</a></li><li><a href="#fom-measurement">FOM Measurement</a></li></ul></li><li><a href="#result-from-optimized-layout-builds">Result from Optimized-Layout Builds</a></li><li><a href="#build-and-testing-instructions">Build and Testing Instructions</a></li></ul></nav><div class="odoc-content"><h3 id="algorithm"><a href="#algorithm" class="anchor"></a>Algorithm</h3><p>The design is based around the 4-step algorithm which decomposes the full 2<sup>24</sup> NTT into multiple 2<sup>12</sup> NTTs across columns and rows of a 2<sup>12</sup> x 2<sup>12</sup> matrix. The 4-step algorithm is described in section 7.1 of <a href="https://arxiv.org/pdf/2011.11524.pdf">this paper</a>. Here's a summary of what it is:</p><p>1. Layout the 2<sup>24</sup> size input data as a 2<sup>12</sup> X 2<sup>12</sup> matrix in row-major order (ie: <code>mat[i][j] = data[i * 2^12 + j]</code>)</p><p>2. Perform a length-2<sup>12</sup> NTT along all columns and write the results back in place</p><p>3. Multiply <code>mat[i][j]</code> by <code>x^{i * j}</code>, where <code>x</code> is the N-th root of unity of the underling field, and N = 2<sup>24</sup></p><p>4. Perform a length-2<sup>12</sup> NTT along all rows and write the results back in place</p><p>5. Tranpose the matrix</p><p>The overall complexity (in terms of butterfly operations performed) is roughly equivalent to a single 2<sup>24</sup> INNT, though an extra twiddle factor correction pass (ie: step 3) is required between the column and row phases.</p><p>On the otherhand, onchip memory usage is drastically reduced, and it becomes possible to implement multiple smaller INNT cores for improved performance through parallelism.</p><h3 id="our-implementation"><a href="#our-implementation" class="anchor"></a>Our Implementation</h3><p>We implemented our NTT core on top of the Vitis platform for Varium C1100. This platform provides us with the PCIe and HBM interfaces. As such the design is provided as Vitis kernels which are put together to provide the final architecture.</p><p>There are 2 Vitis kernels involved in our implementation:</p><ul><li>Hardcaml RTL Kernel implementing the core NTT algorithm</li><li>C++ HLS Kernel which sequences PCIe and HBM memory accesses</li></ul> <img src="https://fyquah.github.io/hardcaml_zprize/assets/ntt-design-overview.png" /><p>Our implementation can be parameterized by the number of cores it supports - the only requirement is it has a power of 2 and there must be at least 8 cores (and subject to resource limits on the FPGA). Each of these cores is capable of performing a 2<sup>12</sup> NTT using on-chip memory.</p><h4 id="2-phases-to-compute-the-ntt"><a href="#2-phases-to-compute-the-ntt" class="anchor"></a>2 Phases to Compute the NTT</h4><p>Our actual NTT implementation comprises of two phases. Let C be the number of parallel NTT cores in our design.</p><p>(in actuality, our design scales by blocks of 8 cores, but it's easier to think in terms of cores)</p><p>Phase 1: Performs steps 1, 2 and 3 of the algorithm</p><ul><li>The HLS kernel streams the first C columns via AXI Stream to the Hardcaml kernel from a HBM bank</li><li>The Hardcaml RTL kernel accepts the NTT</li><li>The HLS kernel concurrently writes the results of the into a different HBM Bank</li></ul><p>Phase 2: Performs step 4 and step 5 of the algorithm simultaneously.</p><ul><li>The HLS kernel stream rows to the hardcaml kernel via Axi Stream</li><li>The Hardcaml RTL kernel perform the NTT and sends the results back via Axi stream</li><li>The HLS kernel concurrently writes it back in columns - this implicitly performs a matrix transpose without the dedicated step</li></ul><p>For a more thorough discussion on the design of individual NTT-cores, please refer the <a href="ntt.html">Hardcaml_ntt</a> page.</p><h3 id="memory-bandwidth-and-streaming"><a href="#memory-bandwidth-and-streaming" class="anchor"></a>Memory Bandwidth and Streaming</h3><p>THe 4 step algorithm requires both a coloumn and row transform, with transposes between phases. This is performed both by controlling the memory access pattern (normal layout build) or by pre and post processing the input/output matrices (optimized layout builds).</p><p>One significant issue we have faced with this project is the bandwidth performance of HBM. In normal layouts, we tend to burst 64 to 512 bytes before opening a new row. The row open operation appears to be taking upto 200-250 HBM clock cycles (about 100 cycles at our internal 200 Mhz clock). We had expected significantly better performance from HBM than this and lacked time to try tuning various HBM parameters to see if we could get better performance.</p><p>The optimized layouts use the host for pre/post processing and dramaticlly improve bandwidth efficiency - the smallest transfers are now 2048 - 4096 bytes (which is only for one read phase - the other read/write phases are completely linear).</p><p>We see tremendously improved throughput of the core with this scheme, though we expect it to be judged harshly in this competition due to the extra pre/post processing step. We include it none-the-less as it shows the potential performance we can get to with either a more optimised HBM structure, or different memory architecture (like DDR4).</p><h3 id="experiments"><a href="#experiments" class="anchor"></a>Experiments</h3><p>To evaluate our results, we perform 2 sets of experiments.</p><h4 id="normal-layout-builds"><a href="#normal-layout-builds" class="anchor"></a>Normal-layout Builds</h4><p>These are builds where the input and output vector to perform NTT on is laid out linearly in HBM (ie: the host doesn't perform any pre/post-processing). We run experiments with running the 8-core, 16-core, 32-core and 64-core variants, yeilding different levels of parallelism.</p><h4 id="optimized-layout-builds"><a href="#optimized-layout-builds" class="anchor"></a>Optimized-layout Builds</h4><p>As discussed in the preceeding section, our performance is significantly bound by bandwidth. We conduct 2 builds (32-core and 64-core variant) with a simple data-rearrangement preprocessing step such that the host can stream data in 2048-4096 byte bursts.</p><h3 id="results-(for-competition-criteria)"><a href="#results-(for-competition-criteria)" class="anchor"></a>Results (For Competition Criteria)</h3><p>We have tested our design and ran builds on a 6-core Intel(R) Core(TM) i5-9600K CPU @ 3.70GHz machine with <code>Ubuntu 22.04 LTS (GNU/Linux 5.15.0-48-generic x86_64)</code>. We did not use any special kernel flags / boot parameters to obtain our results. We run our designs using the Vitis platform Xilinx has provided for the Varium C1100 card. The platform takes up some resources on the FPGA and comes with PCIe gen3 x4 support</p><p>We measured our latency by taking the FPGA-only evaluation latency across 200 NTT runs. Power was measured by sampling <code>xbutil examaine --report electrical
--device &lt;device-pcie-id&gt;</code> 10 times during the latency benchmark.</p><p>In this normal layout build, we do not perform any preprocessing or post-processing. Hence, latency below includes only the FPGA NTT evaluation latency.</p><h4 id="latency,-power-and-resource-utilization"><a href="#latency,-power-and-resource-utilization" class="anchor"></a>Latency, Power and Resource Utilization</h4><p>The table below depicits our results for various builds</p><pre><code>|------------------------------------------------------------------------------|
|   Build | Latency(s) | Power(W) | LUTS   | Registers |  DSP | BRAM36 | URAM  |
|------------------------------------------------------------------------------|
|  8 core |     0.2315 |    16.97 | 107291 |    141006 |  260 |    162 |   48  |
| 16 core |     0.1238 |    18.19 | 126422 |    156149 |  512 |    162 |   96  |
| 32 core |     0.0691 |    21.13 | 166488 |    184436 | 1028 |    162 |   192 |
| 64 core |     0.0450 |    27.70 | 265523 |    246385 | 2052 |    162 |   384 |
|------------------------------------------------------------------------------|</code></pre><p>Here are the available resources on the FPGA. Note that as we are building on top of a Vitis platform, it imposes a non-trivial fixed-cost that we don't control. The number is reported as &quot;fixed&quot; in the post_route_utilization.rpt</p><pre><code>----------------------------------------------------------
| Resource  | Available on FPGA | Used by Vitis Platform |
----------------------------------------------------------
|      LUTS |            870720 |                  62191 |
| Registers |           1743360 |                  81502 |
|       DSP |              5952 |                      4 |
|    BRAM36 |              1344 |                      0 |
|      URAM |               640 |                      0 |
----------------------------------------------------------</code></pre><h4 id="fom-measurement"><a href="#fom-measurement" class="anchor"></a>FOM Measurement</h4><p>Here are our FOM numbers. As detailed in the evaluation criteria given to us, FOM is computed as <code>latency * sqrt(Power) * U_norm</code>. Note that <code>N_pipe = 1</code> for our design, since it can only support 1 evaluation at a time.</p><p>Latency and Power is used as report above in seconds and Watts respectively. We calculate <code>U_norm = U(LUTS) + U(Registers) + U(DSP) + U(BRAM) + U(URAM)</code>. The max possible value of <code>U_norm</code> is hence 4.0, since 0 &lt;= U(.) &lt; 1.0</p><p>These are FOM numbers assuming we don't include the platform (aka fixed resources) in our utlization</p><pre><code>------------------------------------------------------------------------------
|                     Utilization (%)                      |        |        |
|  Build  |  LUTs  |  Registers |    DSP |   BRAM |   URAM | U_norm |  FOM   |
------------------------------------------------------------------------------
| 8-core  | 0.0518 |     0.0341 | 0.0430 | 0.1205 | 0.0750 | 0.3245 | 0.3095 |
| 16-core | 0.0749 |     0.0428 | 0.0860 | 0.1205 | 0.1500 | 0.4743 | 0.2505 |
| 32-core | 0.1198 |     0.0590 | 0.1720 | 0.1205 | 0.3000 | 0.7714 | 0.2451 |
| 64-core | 0.2335 |     0.0946 | 0.3441 | 0.1205 | 0.6000 | 1.3927 | 0.3301 |
------------------------------------------------------------------------------</code></pre><p>Our best-build for the evaluation criteria is the 32-core variant, with a <b>FOM of 0.2451</b></p><p>The following FOM numbers are assuming we have to include the Vitis platform resources as part of our utilization. To stress this fact -- we don't think those resources should be considered as part of the evaluation!</p><pre><code>------------------------------------------------------------------------------
|         |             Utilization (%)                    |        |        |
|  Build  |   LUTs |  Registers |    DSP |   BRAM |   URAM | U_norm |  FOM   |
------------------------------------------------------------------------------
| 8-core  | 0.1232 |     0.0809 | 0.0437 | 0.1205 | 0.0750 | 0.4433 | 0.4229 |
| 16-core | 0.1463 |     0.0896 | 0.0867 | 0.1205 | 0.1500 | 0.5931 | 0.3132 |
| 32-core | 0.1912 |     0.1058 | 0.1727 | 0.1205 | 0.3000 | 0.8903 | 0.2829 |
| 64-core | 0.3049 |     0.1413 | 0.3448 | 0.1205 | 0.6000 | 1.5116 | 0.3583 |
------------------------------------------------------------------------------</code></pre><p>Using these criteria, our best build is also the 32-core variant with a FOM of 0.2829</p><h3 id="result-from-optimized-layout-builds"><a href="#result-from-optimized-layout-builds" class="anchor"></a>Result from Optimized-Layout Builds</h3><p>Here is a detailed breakdown of a runtime sample of an optimized 64-core build: (The power and utilization is similar to the normal-layout builds)</p><pre><code>// Breakdown of a 2^24 optimized-layout 64-core evaluation

----------------------------------------------------
|               Task                     |   Time  |
----------------------------------------------------
| Preprocessing data rearrangement       | 0.0213s |
| Copying input points to device         | 0.0414s |
| Doing Actual NTT work                  | 0.0267s |  (vs 0.0450s in normal layout)
| Copying final result to host           | 0.0552s |
| Copy from internal page-aligned buffer | 0.0231s |
|----------------------------------------|---------|
| Evaluate NTT                           | 0.1680s |
-----------------------------------------|----------

// Breakdown of a 2^24 optimized-layout 32-core evaluation

----------------------------------------------------
|               Task                     |   Time  |
----------------------------------------------------
| Preprocessing data rearrangement       | 0.0217s |
| Copying input points to device         | 0.0416s |
| Doing Actual NTT work                  | 0.0349s |  (vs 0.0691s in normal layout)
| Copying final result to host           | 0.0554s |
| Copy from internal page-aligned buffer | 0.0228s |
|----------------------------------------|---------|
| Evaluate NTT                           | 0.1770s |
-----------------------------------------|----------</code></pre><p>By rearranging the data in a much more memory-friendly layout, our NTT evaluation time drops significantly compared to those of a 64-core build in a normal build (0.0267s vs 0.0450s). This comes at the cost of the host doing some data rearrangement.</p><p>The bottleneck of our evaluation clear lies in the host and PCIe latency in this result, both of which can be solved pretty easily:</p><ul><li><b>preprocessing + postprocessing &gt; latency</b> - We can run the preprocessing and post-processing in separate threads. We can setup the input and output buffers such such that we don't run into cache coherency issues. We can also mask some of the preprocessing latency with PCIe latency.</li><li><b>The PCIe latency is larger than the NTT evaluation</b> - This is because the vitis platform we are using only supports PCIe x4. With PCIe x16, we would have 4 times the bandwidth and side-step this problem.</li></ul><p>In practice, we believe this is the more scalable design that can achieve low-latency and high-throughput, at the cost of the host machine doing some data rearrangement.</p><h3 id="build-and-testing-instructions"><a href="#build-and-testing-instructions" class="anchor"></a>Build and Testing Instructions</h3><p>Please refer <a href="zprize_ntt_build_instructions.html">this page</a></p></div></body></html>